{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@niveditha.itengineer/learn-how-to-setup-portaudio-and-pyaudio-in-ubuntu-to-play-with-speech-recognition-8d2fff660e94\n",
    "\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import timeit\n",
    "import python_speech_features\n",
    "import os\n",
    "import json\n",
    "#import RPi.GPIO as GPIO\n",
    "from kws_streaming.data import input_data\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#from tflite_runtime.interpreter import Interpreter\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/disk0shared/bdjola/Documents/Technique/DevML/kws_streaming2/kws_streaming'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "#debug_time = 1\n",
    "#debug_acc = 0\n",
    "#led_pin = 8\n",
    "word_threshold = 0.7\n",
    "\n",
    "sample_rate = 16000\n",
    "\n",
    "num_channels = 1\n",
    "model_path = 'models3_30k/svdf/tflite_stream_state_external/stream_state_external.tflite'\n",
    "\n",
    "\n",
    "# GPIO \n",
    "#GPIO.setwarnings(False)\n",
    "#GPIO.setmode(GPIO.BOARD)\n",
    "#GPIO.setup(8, GPIO.OUT, initial=GPIO.LOW)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "DATA_PATH = os.path.join(current_dir, \"data2/\")\n",
    "MODELS_PATH = current_dir\n",
    "\n",
    "file_name='models3_30k'\n",
    "train_dir = os.path.join(MODELS_PATH, file_name, 'svdf')\n",
    "\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is another way of reading flags - through json\n",
    "with tf.compat.v1.gfile.Open(os.path.join(train_dir, 'flags.json'), 'r') as fd:\n",
    "   flags_json = json.load(fd)\n",
    "\n",
    "class DictStruct(object):\n",
    "   def __init__(self, **entries):\n",
    "     self.__dict__.update(entries)\n",
    "\n",
    "flags = DictStruct(**flags_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 19:15:41.634687: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-02-25 19:15:41.634718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-02-25 19:15:41.634722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{16: 'learn',\n",
       " 11: 'forward',\n",
       " 10: 'follow',\n",
       " 2: 'backward',\n",
       " 3: 'bed',\n",
       " 5: 'cat',\n",
       " 14: 'happy',\n",
       " 4: 'bird',\n",
       " 18: 'marvin',\n",
       " 6: 'dog',\n",
       " 15: 'house',\n",
       " 30: 'tree',\n",
       " 12: 'four',\n",
       " 32: 'up',\n",
       " 24: 'right',\n",
       " 21: 'off',\n",
       " 8: 'eight',\n",
       " 17: 'left',\n",
       " 22: 'on',\n",
       " 13: 'go',\n",
       " 23: 'one',\n",
       " 33: 'visual',\n",
       " 7: 'down',\n",
       " 19: 'nine',\n",
       " 20: 'no',\n",
       " 9: 'five',\n",
       " 26: 'sheila',\n",
       " 34: 'wow',\n",
       " 29: 'three',\n",
       " 35: 'yes',\n",
       " 36: 'zero',\n",
       " 28: 'stop',\n",
       " 27: 'six',\n",
       " 31: 'two',\n",
       " 25: 'seven',\n",
       " 0: '_silence_'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare mapping of index to word\n",
    "audio_processor = input_data.AudioProcessor(flags)\n",
    "index_to_label = {}\n",
    "# labels used for training\n",
    "for word in audio_processor.word_to_index.keys():\n",
    "  if audio_processor.word_to_index[word] == data.input_data.SILENCE_INDEX:\n",
    "    index_to_label[audio_processor.word_to_index[word]] = data.input_data.SILENCE_LABEL\n",
    "  elif audio_processor.word_to_index[word] == data.input_data.UNKNOWN_WORD_INDEX:\n",
    "    index_to_label[audio_processor.word_to_index[word]] = data.input_data.UNKNOWN_WORD_LABEL\n",
    "  else:\n",
    "    index_to_label[audio_processor.word_to_index[word]] = word\n",
    "\n",
    "# training labels\n",
    "index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'input_audio', 'index': 0, 'shape': array([  1, 320], dtype=int32), 'shape_signature': array([  1, 320], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'streaming/speech_features/data_frame_1input_state', 'index': 1, 'shape': array([  1, 640], dtype=int32), 'shape_signature': array([  1, 640], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'streaming/svdf_0/depthwise_conv1d_6/depthwise_conv1d_6input_state', 'index': 2, 'shape': array([ 1,  4, 16], dtype=int32), 'shape_signature': array([ 1,  4, 16], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'streaming/svdf_1/depthwise_conv1d_7/depthwise_conv1d_7input_state', 'index': 3, 'shape': array([ 1, 10, 32], dtype=int32), 'shape_signature': array([ 1, 10, 32], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'streaming/svdf_2/depthwise_conv1d_8/depthwise_conv1d_8input_state', 'index': 4, 'shape': array([ 1, 10, 32], dtype=int32), 'shape_signature': array([ 1, 10, 32], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'streaming/svdf_3/depthwise_conv1d_9/depthwise_conv1d_9input_state', 'index': 5, 'shape': array([ 1, 10, 32], dtype=int32), 'shape_signature': array([ 1, 10, 32], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'streaming/svdf_4/depthwise_conv1d_10/depthwise_conv1d_10input_state', 'index': 6, 'shape': array([ 1, 10, 64], dtype=int32), 'shape_signature': array([ 1, 10, 64], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'streaming/svdf_5/depthwise_conv1d_11/depthwise_conv1d_11input_state', 'index': 7, 'shape': array([  1,  10, 128], dtype=int32), 'shape_signature': array([  1,  10, 128], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'streaming/stream/stream/input_state', 'index': 8, 'shape': array([  1,   1, 128], dtype=int32), 'shape_signature': array([  1,   1, 128], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "# Load model (interpreter)\n",
    "#interpreter = Interpreter(model_path)\n",
    "interpreter = tf.lite.Interpreter(model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(input_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1, 320], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = []\n",
    "for s in range(len(input_details)):\n",
    "  inputs.append(np.zeros(input_details[s]['shape'], dtype=np.float32))\n",
    "\n",
    "input_details[0]['shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decimate (filter and downsample)\n",
    "def decimate(signal, old_fs, new_fs):\n",
    "    \n",
    "    # Check to make sure we're downsampling\n",
    "    if new_fs > old_fs:\n",
    "        print(\"Error: target sample rate higher than original\")\n",
    "        return signal, old_fs\n",
    "    \n",
    "    # We can only downsample by an integer factor\n",
    "    dec_factor = old_fs / new_fs\n",
    "    if not dec_factor.is_integer():\n",
    "        print(\"Error: can only decimate by integer factor\")\n",
    "        return signal, old_fs\n",
    "\n",
    "    # Do decimation\n",
    "    resampled_signal = scipy.signal.decimate(signal, int(dec_factor))\n",
    "\n",
    "    return resampled_signal, new_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gets called every 0.5 seconds\n",
    "def sd_callback(rec, frames, time, status):   \n",
    "    #print('rec:', rec.shape) \n",
    "    # Remove 2nd dimension from recording sample\n",
    "    #rec = np.squeeze(rec)\n",
    "    rec = np.reshape(rec, (1,-1))\n",
    "\n",
    "    # set input audio data (by default input data at index 0)\n",
    "    interpreter.set_tensor(input_details[0]['index'], rec)\n",
    "\n",
    "    # set input states (index 1...)\n",
    "    for s in range(1, len(input_details)):\n",
    "        interpreter.set_tensor(input_details[s]['index'], inputs[s])\n",
    "\n",
    "    # run inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # get output: classification\n",
    "    out_tflite = interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(start / 16000.0, np.argmax(out_tflite), np.max(out_tflite))\n",
    "\n",
    "    # get output states and set it back to input states\n",
    "    # which will be fed in the next inference cycle\n",
    "    for s in range(1, len(input_details)):\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "        inputs[s] = interpreter.get_tensor(output_details[s]['index'])\n",
    "\n",
    "    out_tflite_argmax = np.argmax(out_tflite)\n",
    "    proba = np.max(out_tflite)\n",
    "    if (out_tflite_argmax>1) and (proba > word_threshold):\n",
    "        print(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), index_to_label[out_tflite_argmax], proba)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1 320]\n",
      "[  1 640]\n",
      "[ 1  4 16]\n",
      "[ 1 10 32]\n",
      "[ 1 10 32]\n",
      "[ 1 10 32]\n",
      "[ 1 10 64]\n",
      "[  1  10 128]\n",
      "[  1   1 128]\n"
     ]
    }
   ],
   "source": [
    "reset_state = True\n",
    "\n",
    "# before processing new test sequence we can reset model state\n",
    "# if we reset model state then it is not real streaming mode\n",
    "if reset_state:\n",
    "  for s in range(len(input_details)):\n",
    "    print(input_details[s]['shape'])\n",
    "    inputs[s] = np.zeros(input_details[s]['shape'], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels: 1 samplerate: 16000 blocksize: 320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From cffi callback <function _StreamBase.__init__.<locals>.callback_ptr at 0x7f32228b6c20>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/disk0shared/bdjola/Documents/Technique/DevML/kws_streaming2/kws_streaming/.venv/lib/python3.7/site-packages/sounddevice.py\", line 846, in callback_ptr\n",
      "    return _wrap_callback(callback, data, frames, time, status)\n",
      "  File \"/mnt/disk0shared/bdjola/Documents/Technique/DevML/kws_streaming2/kws_streaming/.venv/lib/python3.7/site-packages/sounddevice.py\", line 2687, in _wrap_callback\n",
      "    callback(*args)\n",
      "  File \"/tmp/ipykernel_5734/4124017927.py\", line 32, in sd_callback\n",
      "TypeError: descriptor 'strftime' requires a 'datetime.date' object but received a 'str'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5734/2352431828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     callback=sd_callback):\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start streaming from microphone\n",
    "print('channels:',num_channels, 'samplerate:',sample_rate, 'blocksize:', flags.window_stride_samples)\n",
    "with sd.InputStream(channels=num_channels,\n",
    "                    samplerate=sample_rate,\n",
    "                    blocksize=flags.window_stride_samples,\n",
    "                    callback=sd_callback):\n",
    "    while True:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
